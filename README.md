# Live_Twitter_Dashboard
This project is a real time insights dashboard for monitoring corporate twitter accounts using python, twitter APIs, the google NLP API, google bigquery, and tableau.

“Live” version is available at http://www.seanconeys.com/projects/Twitter_Dash.html
Note: Version is updated daily but is not truly live since I don’t have a tableau server license and tableau public doesn't support live updates. Additionally, while it would be very easy to stream live tweet tweet data 24/7, I only turn it on briefly every half hour to ensure I don’t exceed my google api quotas.

Jupyter Notebooks often have issues loading on github. If issues occur the address of the notebook can be copied and pasted into the notebook viewer on the jupyter website: https://nbviewer.jupyter.org/

The python has two programs. Tweet_Data_Script handles and process a live streams of tweets from twitter. Engagement_Data_Script makes a best effort to calculate engagement metrics using the twitter search api. Generally companies track engagement through google analytics and Twitters engagement API, however I don’t have access to a company's analytics account or enterprise level Twitter APIs so this isn’t an option. In the absence of these tools many KPIs like follower count, tweet favorites, retweets, and replies can be calculated but some KPIs such as conversion rates are impossible to calculate without access to one of these. After data is prepared by one of these scripts it is stored in a google bigquery DB. Our tableau workbook is connected to our bigquery DB which it then pulls from using a combination of traditional tableau data importing and custom sql queries to create our graphs. 

Database_Init_Script:
Initializes the bigquery tables for our project. Table Schemas can be viewed here.

Tweet Class:
We create a class that on init gets passed tweet_json. A tweet object is designed to then have its prepare_$TWEETTYPE() and store_$TWEETTYPE()  methods called by a handler depending on the type of tweet being handled. The prepare_$TWEETTYPE() method calls internal functions to extract the data from the json and make any necessary API calls (e.g google NLP). The store_$TWEETTYPE() methods then take this data and formats it according to our schema and then stores it to the proper table. 

NOTES: Bigquery only supports 100 update operations a day. So when we want to update the number of favorites and retweets on our accounts most recent status it would make logical sense to perform an update on our records. However if we did this more than 4 times an hour we would exceed our daily limit on update operations, therefore it makes sense to simply insert a new row into our DB instead of doing an update on it. This however creates a new problem because we want to be able to filter for our most recent update and since it is the same tweet its created_at field will be the same for all entries. The answer is to create our table with a primary_id field, however this isn’t supported natively in bigquery either since bigquery is intended to be used as a data lake. So instead we have an sql query that returns the highest primary_id in our table and we then add 1 to it to get our new row’s primary_id. If our query returns nothing it means our DB is empty and we return 0 as a default.  In reality this means that bigquery probably isn’t the best choice for a DB for this project however bigquery was chosen because the main purpose of this project is to display my ability to use bigquery to potential employers.


Tweet_Data_Script:
First we configure our logging settings then we setup our authentication for our APIs and our DB table ids and client.

We then use the tweepy wrapper for the twitter api to create a new stream listener. This stream listener waits until a tweet matches its filter criteria and then executes on_status(). Our on_status() function creates a tweet object from the tweet. The tweet is then processed and stored by the tweet object. Our filter is then initialized to filter for english tweets about our company.

NOTE: While this could easily (and ideally) be run 24/7 I have it configured to sample tweet data every 30 min so that I don’t exceed my free usage limit for my google APIs.


Engagement_Data_Script:
Since this is designed to workaround lack of access to better tools it gets a bit tricky. First we update our follower count. We make a call to the twitter api for our account and store the followers count in the DB. We then want to find our accounts most recent status and replies to it. This could be implemented by matching conditions from a live stream, however any downtime could result in incorrect data. Since to prevent charges my stream is not always active and since any company using this would use a different tool to calculate these measures it made sense to implement it using a search instead. First we find the id of our most recent status by iterating over our account most recent tweets and checking the json data until we find a tweet that isn’t a retweet a quote or a response to another tweet. We then store it and the engagement data from its json to our DB. This data will contain retweets and favorites information for the tweet but it will not contain info on replies to the tweet. To find this we need to search the most recent tweets made @ our account and find tweets where the in_response_to id matches the id of our last status. Our dashboard only uses data for the 10 most recent tweets so we also need to look for cases where it matches the id of any of our last 10 statuses. We don’t want to search tweets twice so were going to filter our search using the since_id parameter meaning that only tweets since that tweet_id will be searched. So we use sql queries to find the ids of our most recent reply and our 10th most recent tweet and then use whichever is larger (e.g more recent) as our filter for since_id. Then the json of any tweets matching our criteria are passed to a tweet object and then prepared and stored. 
NOTE: If run often enough this should work well, however the search has a limit on requests in a given time period. Therefore if tweet volume is too big to be handled in one query it will return a 429 response before all data is parsed. Because the search api returns most recent results first this can result in gaps in our data because the tweet_id we filter on the next time we query will be larger than that of some of the tweets we haven’t seen. 


Dashboard:
First 3 KPI sliders indicate performance of most recent corporate status relative to the last ten statuses. All the way left on the slider indicates it is the worst performing tweet of the last ten for that metric and all the way on the right indicates it is the best performing tweet of the last ten by that metric. The last slider for sentiment simply shows the avg sentiment of response to the latest status on a scale from -1 to 1. These were very difficult to create as bullet graphs relative to a changing min and max value aren’t really supported in tableau and I couldn’t find anyone else online who had been able to implement them. Through some fanangeling I was able to implement them by graphing the percent returned by the formula 
(last_tweet.value - min(last_10_tweets.values)) /max(last_10_tweets.values)
