{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from datetime import datetime, timedelta\n",
    "from google.oauth2 import service_account\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.cloud import bigquery\n",
    "import schedule\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('walmart')\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='main_twitter_dash.log', filemode='w',level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authentication(cons_key, cons_secret, acc_token, acc_secret):\n",
    "    auth = tweepy.OAuthHandler(cons_key, cons_secret)\n",
    "    auth.set_access_token(acc_token, acc_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API Setup\n",
    "ACC_TOKEN = 'YOUR INFO HERE'\n",
    "ACC_SECRET = 'YOUR INFO HERE'\n",
    "CONS_KEY = 'YOUR INFO HERE'\n",
    "CONS_SECRET = 'YOUR INFO HERE'\n",
    "\n",
    "api = authentication(CONS_KEY,CONS_SECRET,ACC_TOKEN,ACC_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Service Account Credentials\n",
    "credentials = service_account.Credentials.from_service_account_file('PATH TO GOOGLE CREDENTIALS')\n",
    "\n",
    "#DBCLIENT INTIALIZATION\n",
    "tweets_table_id = \"TABLE NAME\"\n",
    "keywords_table_id = \"TABLE NAME\"\n",
    "hashtags_table_id = \"TABLE NAME\"\n",
    "company_tweets_table_id = \"TABLE NAME\"\n",
    "replies_table_id = \"TABLE NAME\"\n",
    "followers_table_id = \"TABLE NAME\"\n",
    "project_id = \"PROJECT NAME\"\n",
    "db_client = bigquery.Client(project=project_id, credentials = credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet:\n",
    "    \n",
    "    def clean_tweet(self, tweet_json):\n",
    "        user_removed = re.sub(r'@[A-Za-z0-9]+','',tweet_json.decode('utf-8'))\n",
    "        link_removed = re.sub('https?://[A-Za-z0-9./]+','',user_removed)\n",
    "        number_removed = re.sub('[^a-zA-Z0-9]', ' ', link_removed)\n",
    "        lower_case_tweet= number_removed.lower()\n",
    "        tok = WordPunctTokenizer()\n",
    "        words = tok.tokenize(lower_case_tweet)\n",
    "        clean_tweet = (' '.join(words)).strip()\n",
    "        return clean_tweet\n",
    "    \n",
    "    def get_hashtags(self, tweet_json):\n",
    "        hashtags = []\n",
    "        if tweet_json.entities['hashtags'] != []:\n",
    "            hashtags = [hashtag[\"text\"] for hashtag in tweet_json.entities['hashtags']]\n",
    "            \n",
    "        return hashtags\n",
    "    \n",
    "    def get_sentiment(self, text):\n",
    "        client = language.LanguageServiceClient(credentials = credentials)\n",
    "        document = types\\\n",
    "                   .Document(content=text,\n",
    "                             type=enums.Document.Type.PLAIN_TEXT)\n",
    "        sentiment= client\\\n",
    "                          .analyze_sentiment(document=document)\\\n",
    "                          .document_sentiment\\\n",
    "                          \n",
    "        return sentiment.score\n",
    "    \n",
    "    def get_keywords(self, text):\n",
    "        keywords = []\n",
    "        \n",
    "        #Verbs, adjs, and nouns\n",
    "        parts_of_speech = [1, 6, 11]\n",
    "        \n",
    "        client = language.LanguageServiceClient(credentials = credentials)\n",
    "        document = types\\\n",
    "                   .Document(content=text,\n",
    "                             type=enums.Document.Type.PLAIN_TEXT)\n",
    "        syntax = client\\\n",
    "                          .analyze_syntax(document=document)\n",
    "        \n",
    "        for token in syntax.tokens:\n",
    "            if token.part_of_speech.tag in parts_of_speech and token.text.content not in stop_words and token.text.content != \"rt\":\n",
    "                keywords.append(token.text.content) \n",
    "                          \n",
    "        return keywords\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, tweet_json):\n",
    "        self.json = tweet_json\n",
    "    \n",
    "    def get_last_company_tweet_primary_id(self):\n",
    "        query = (\"\"\"\n",
    "            SELECT MAX(primary_id)\n",
    "            FROM `TABLE NAME`\n",
    "            \"\"\"\n",
    "            )\n",
    "        query_job = db_client.query(\n",
    "                query\n",
    "            )  \n",
    "\n",
    "        results = query_job.result()\n",
    "\n",
    "        for row in results:\n",
    "            if str(row[0]) == 'None':\n",
    "                return(1)\n",
    "            else:\n",
    "                return(row[0])\n",
    "        \n",
    "   \n",
    "\n",
    "        \n",
    "    def prepare_tweet(self):\n",
    "        self.text = self.clean_tweet(self.json.text.encode('utf-8'))\n",
    "        self.created_at = datetime.timestamp(self.json.created_at)\n",
    "        self.hashtags = self.get_hashtags(self.json)\n",
    "        self.sentiment = self.get_sentiment(self.text)\n",
    "        self.keywords = self.get_keywords(self.text)\n",
    "        self.primary_id = self.json.id\n",
    "        \n",
    "    def prepare_company_tweet(self):\n",
    "        self.text = self.json.text\n",
    "        self.created_at = datetime.timestamp(self.json.created_at)\n",
    "        self.tweet_id = self.json.id\n",
    "        self.primary_id = (self.get_last_company_tweet_primary_id() + 1)\n",
    "        self.favorites = self.json.favorite_count\n",
    "        self.retweets = self.json.retweet_count\n",
    "        \n",
    "    def prepare_reply(self):\n",
    "        self.text = self.clean_tweet(self.json.text.encode('utf-8'))\n",
    "        self.created_at = datetime.timestamp(self.json.created_at)\n",
    "        self.sentiment = self.get_sentiment(self.text)\n",
    "        self.primary_id = self.json.id\n",
    "        self.response_id = self.json.in_reply_to_status_id\n",
    "        \n",
    "    def store_tweet(self):\n",
    "        errors = []\n",
    "       \n",
    "        tweet_rows = [[self.primary_id, self.created_at, self.text, self.sentiment]]\n",
    "        table = db_client.get_table(tweets_table_id)\n",
    "        errors.append(db_client.insert_rows(table, tweet_rows))\n",
    "        \n",
    "        if self.keywords != []:\n",
    "            keyword_rows = [[self.created_at, keyword, self.primary_id] for keyword in self.keywords]\n",
    "            table = db_client.get_table(keywords_table_id)\n",
    "            errors.append(db_client.insert_rows(table, keyword_rows))\n",
    "            \n",
    "        if self.hashtags != []:\n",
    "            hashtag_rows = [[self.created_at, hashtag, self.primary_id] for hashtag in self.hashtags]\n",
    "            table = db_client.get_table(hashtags_table_id)\n",
    "            errors.append(db_client.insert_rows(table, hashtag_rows))\n",
    "        \n",
    "        if errors == True:\n",
    "            logging.error(\"Tweet Storage Error\" + errors)\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def store_company_tweet(self):\n",
    "        errors = []\n",
    "       \n",
    "        company_tweets_rows = [[self.primary_id, self.tweet_id, self.created_at, self.text, self.favorites, self.retweets]]\n",
    "        table = db_client.get_table(company_tweets_table_id)\n",
    "        errors.append(db_client.insert_rows(table, company_tweets_rows))\n",
    "    \n",
    "        if errors == True:\n",
    "            logging.error(\"Company Tweet Storage Error\" + errors)\n",
    "            \n",
    "        return errors\n",
    "    \n",
    "    def store_reply(self):\n",
    "        errors = []\n",
    "       \n",
    "        reply_rows = [[self.primary_id, self.response_id, self.created_at, self.text, self.sentiment ]]\n",
    "        table = db_client.get_table(replies_table_id)\n",
    "        errors.append(db_client.insert_rows(table, reply_rows))\n",
    "        \n",
    "        if errors == True:\n",
    "            logging.error(\"Reply Storage Error\" + errors)\n",
    "            \n",
    "        return errors\n",
    "    \n",
    "    def get_data(self):\n",
    "        print(self.text)\n",
    "        print(self.created_at)\n",
    "        print(self.hashtags)\n",
    "        print(self.sentiment)\n",
    "        print(self.keywords)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        tweet_handler = tweet(status)\n",
    "        tweet_handler.prepare_tweet()\n",
    "        tweet_handler.store_tweet()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stream():\n",
    "    logging.info(\"Tweet Stream Started\")\n",
    "\n",
    "    try:\n",
    "        myStreamListener = MyStreamListener()\n",
    "        myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "        myStream.filter(track=['walmart'], languages=['en'], is_async = True)\n",
    "        time.sleep(20)\n",
    "        myStream.disconnect()\n",
    "        logging.info(\"Tweet Stream Closed\")\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\"Streaming Error\" + str(e))\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.every(30).minutes.do(run_stream) \n",
    "\n",
    "while True: \n",
    "    try:\n",
    "        schedule.run_pending() \n",
    "        time.sleep(1) \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\"Scheduler Failure\" + str(e))\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
